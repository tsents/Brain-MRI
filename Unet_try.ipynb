{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "PATH = \"kaggle_3m\"\n",
    "IMAGE_SIZE = (256, 256)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "paths = []\n",
    "\n",
    "for dirname in os.listdir(PATH):\n",
    "    if os.path.isdir(os.path.join(PATH, dirname)):\n",
    "        \n",
    "        for filename in os.listdir(os.path.join(PATH, dirname)):\n",
    "            if filename.endswith('.tif'):\n",
    "                paths.append(PATH+'/'+dirname+'/'+filename)\n",
    "\n",
    "\n",
    "def data_frame(data):\n",
    "    images = list(filter(lambda x: not x.endswith('mask.tif'), data))\n",
    "    images.sort(key=lambda x: int(x.rsplit('_', 3)[-1][:-4]))\n",
    "    images.sort(key=lambda x: int(x.rsplit('_', 3)[-2]))\n",
    "    \n",
    "    IDs = list(map(lambda x: x.rsplit('/', 3)[-1][:-4], images))\n",
    "\n",
    "    masks = list(filter(lambda x: x.endswith('mask.tif'), data))\n",
    "    masks.sort(key=lambda x: int(x.rsplit('_', 3)[-2]))\n",
    "    masks.sort(key=lambda x: int(x.rsplit('_', 3)[-3]))\n",
    "\n",
    "    pixels = lambda x: Image.open(x)\n",
    "    largest_pixel = lambda y: np.max(pixels(y))\n",
    "    diagnotic_function = lambda z: 1 if largest_pixel(z) > 0 else 0\n",
    "    diagnoses = list(map(lambda x: diagnotic_function(x), masks))\n",
    "\n",
    "    data_df = pd.DataFrame({'ID': IDs, 'Image': images, 'Mask': masks, 'Diagnosis': diagnoses})\n",
    "    \n",
    "    train_index, val_index = train_test_split(data_df.index.values.tolist(), test_size=0.19, random_state=42)\n",
    "    val_index, test_index = train_test_split(val_index, test_size=0.12, random_state=42)\n",
    "    \n",
    "    # Making train, test, and validation dataframes\n",
    "    train_df, val_df, test_df = data_df.iloc[train_index], data_df.iloc[val_index], data_df.iloc[test_index]\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "    \n",
    "# Making the dataframes\n",
    "train_df, val_df, test_df = data_frame(paths)\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "print(train_df.head())\n",
    "\n",
    "def tensor_from_path(path):\n",
    "    arr = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    arr = cv2.resize(arr, IMAGE_SIZE)\n",
    "    arr = arr / 255\n",
    "    if len(arr.shape) == 3:\n",
    "        tensor = torch.tensor(arr).permute(2,0,1)\n",
    "    elif len(arr.shape) == 2:\n",
    "        tensor = torch.tensor(arr).unsqueeze(0)\n",
    "    else:\n",
    "        raise ValueError(f\"Expected data shape to be (..., ..., 3) or (..., ...) , but got {arr.shape}\")\n",
    "    return tensor\n",
    "\n",
    "class data(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super().__init__()\n",
    "        self.df = df        \n",
    "        self.images = self.df.loc[:,'Image'].values\n",
    "        self.masks = self.df.loc[:,'Mask'].values\n",
    "        self.diagnosis = self.df.loc[:,'Diagnosis'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        im_path = self.images[idx]\n",
    "        msk_path= self.masks[idx]\n",
    "        diagnosis = self.diagnosis[idx]\n",
    "        self.im_tensor = tensor_from_path(im_path)\n",
    "        self.msk_tensor= tensor_from_path(msk_path)\n",
    "        return self.im_tensor.to(device), self.msk_tensor.to(device),diagnosis\n",
    "\n",
    "train_ds = data(train_df)\n",
    "test_ds = data(test_df)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32)\n",
    "test_dl = DataLoader(test_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
